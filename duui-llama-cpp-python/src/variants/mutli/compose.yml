version: '3'
services:
  llama_cpp_python:
    build: 
      context: .
      dockerfile: Dockerfile
    ports:
      - 9714:9714
    volumes:
      - /storage/nlp/model/gguf/:/app/models/
      - /storage/nlp/huggingface/:/root/.cache/huggingface/
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: [gpu]
            device_ids: ['0', '1']
